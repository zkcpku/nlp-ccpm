12/17/2021 21:22:12 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
12/17/2021 21:22:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/runs/Dec17_21-22-11_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
output_dir=tmp/,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=59,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
12/17/2021 21:22:13 - WARNING - datasets.builder - Using custom data configuration default-00f67695a454f3a9
12/17/2021 21:22:13 - INFO - datasets.builder - Overwrite dataset info from restored data version.
12/17/2021 21:22:13 - INFO - datasets.info - Loading Dataset info from /home/zhangkechi/.cache/huggingface/datasets/json/default-00f67695a454f3a9/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426
12/17/2021 21:22:13 - WARNING - datasets.builder - Reusing dataset json (/home/zhangkechi/.cache/huggingface/datasets/json/default-00f67695a454f3a9/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426)
12/17/2021 21:22:13 - INFO - datasets.info - Loading Dataset info from /home/zhangkechi/.cache/huggingface/datasets/json/default-00f67695a454f3a9/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 456.70it/s]
[INFO|configuration_utils.py:604] 2021-12-17 21:22:17,627 >> loading configuration file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/config.json from cache at /home/zhangkechi/.cache/huggingface/transformers/1cd8aef41c425344f501c4072d646a95424c930385ee3568506cb54e66331d22.518e5ac3dc0dc1cb2c76aa391bc23d96ca5a5bc6b50df9ba9a411b3cd58f93d1
[INFO|configuration_utils.py:641] 2021-12-17 21:22:17,629 >> Model config RobertaConfig {
  "_name_or_path": "ethanyt/guwenbert-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "tokenizer_class": "BertTokenizer",
  "transformers_version": "4.14.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 23292
}

[INFO|tokenization_auto.py:352] 2021-12-17 21:22:19,123 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:604] 2021-12-17 21:22:22,492 >> loading configuration file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/config.json from cache at /home/zhangkechi/.cache/huggingface/transformers/1cd8aef41c425344f501c4072d646a95424c930385ee3568506cb54e66331d22.518e5ac3dc0dc1cb2c76aa391bc23d96ca5a5bc6b50df9ba9a411b3cd58f93d1
[INFO|configuration_utils.py:641] 2021-12-17 21:22:22,493 >> Model config RobertaConfig {
  "_name_or_path": "ethanyt/guwenbert-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "tokenizer_class": "BertTokenizer",
  "transformers_version": "4.14.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 23292
}

[INFO|tokenization_utils_base.py:1742] 2021-12-17 21:22:35,053 >> loading file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/vocab.txt from cache at /home/zhangkechi/.cache/huggingface/transformers/c1c6d07618ea36c7539c9d4434f35a976acad8c83419b259ef0479e07f696148.2d266873baa0d87053e94d3726b56b4a2d050af472c9872b09cd7241538d2064
[INFO|tokenization_utils_base.py:1742] 2021-12-17 21:22:35,054 >> loading file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2021-12-17 21:22:35,054 >> loading file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2021-12-17 21:22:35,054 >> loading file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2021-12-17 21:22:35,055 >> loading file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:604] 2021-12-17 21:22:38,298 >> loading configuration file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/config.json from cache at /home/zhangkechi/.cache/huggingface/transformers/1cd8aef41c425344f501c4072d646a95424c930385ee3568506cb54e66331d22.518e5ac3dc0dc1cb2c76aa391bc23d96ca5a5bc6b50df9ba9a411b3cd58f93d1
[INFO|configuration_utils.py:641] 2021-12-17 21:22:38,300 >> Model config RobertaConfig {
  "_name_or_path": "ethanyt/guwenbert-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "tokenizer_class": "BertTokenizer",
  "transformers_version": "4.14.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 23292
}

[INFO|configuration_utils.py:604] 2021-12-17 21:22:41,738 >> loading configuration file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/config.json from cache at /home/zhangkechi/.cache/huggingface/transformers/1cd8aef41c425344f501c4072d646a95424c930385ee3568506cb54e66331d22.518e5ac3dc0dc1cb2c76aa391bc23d96ca5a5bc6b50df9ba9a411b3cd58f93d1
[INFO|configuration_utils.py:641] 2021-12-17 21:22:41,740 >> Model config RobertaConfig {
  "_name_or_path": "ethanyt/guwenbert-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "tokenizer_class": "BertTokenizer",
  "transformers_version": "4.14.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 23292
}

[INFO|modeling_utils.py:1352] 2021-12-17 21:22:43,273 >> loading weights file https://huggingface.co/ethanyt/guwenbert-base/resolve/main/pytorch_model.bin from cache at /home/zhangkechi/.cache/huggingface/transformers/86f1ada43367e7d6fc2730a96178f720a528284a4b41cc0edffa832fe504eb0b.ab8d267dfb48d8610c457d4ebb67b511360984a8e2cbfe7e1e01c72ca49244fa
[WARNING|modeling_utils.py:1611] 2021-12-17 21:22:44,789 >> Some weights of the model checkpoint at ethanyt/guwenbert-base were not used when initializing BinaryRobertaChoice: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing BinaryRobertaChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BinaryRobertaChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1622] 2021-12-17 21:22:44,790 >> Some weights of BinaryRobertaChoice were not initialized from the model checkpoint at ethanyt/guwenbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/17/2021 21:22:44 - WARNING - __main__ - change roberta to bert
12/17/2021 21:22:47 - WARNING - __main__ - load model from save/binary/guwen/pytorch_model.bin
12/17/2021 21:22:47 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.
12/17/2021 21:22:47 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/zhangkechi/.cache/huggingface/datasets/json/default-00f67695a454f3a9/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426/cache-befe5f383bbc5a87.arrow
12/17/2021 21:22:47 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:549] 2021-12-17 21:22:47,428 >> The following columns in the test set  don't have a corresponding argument in `BinaryRobertaChoice.forward` and have been ignored: origin_idx.
[WARNING|training_args.py:959] 2021-12-17 21:22:47,430 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2281] 2021-12-17 21:22:47,431 >> ***** Running Prediction *****
[INFO|trainer.py:2283] 2021-12-17 21:22:47,431 >>   Num examples = 24797
[INFO|trainer.py:2286] 2021-12-17 21:22:47,431 >>   Batch size = 50
  0%|          | 0/496 [00:00<?, ?it/s]  1%|          | 3/496 [00:00<00:20, 24.07it/s]  1%|          | 6/496 [00:00<00:22, 21.40it/s]  2%|▏         | 9/496 [00:00<00:21, 22.23it/s]  2%|▏         | 12/496 [00:00<00:23, 20.92it/s]  3%|▎         | 15/496 [00:00<00:23, 20.12it/s]  4%|▎         | 18/496 [00:00<00:25, 18.75it/s]  4%|▍         | 20/496 [00:01<00:25, 18.74it/s]  5%|▍         | 23/496 [00:01<00:24, 19.44it/s]  5%|▌         | 25/496 [00:01<00:25, 18.46it/s]  5%|▌         | 27/496 [00:01<00:25, 18.05it/s]  6%|▌         | 30/496 [00:01<00:24, 19.41it/s]  7%|▋         | 33/496 [00:01<00:21, 21.18it/s]  7%|▋         | 36/496 [00:01<00:22, 20.22it/s]  8%|▊         | 39/496 [00:02<00:24, 18.35it/s]  8%|▊         | 41/496 [00:02<00:25, 17.70it/s]  9%|▊         | 43/496 [00:02<00:24, 18.14it/s]  9%|▉         | 45/496 [00:02<00:25, 17.92it/s]  9%|▉         | 47/496 [00:02<00:25, 17.81it/s] 10%|█         | 50/496 [00:02<00:23, 19.24it/s] 10%|█         | 52/496 [00:02<00:23, 18.62it/s] 11%|█         | 55/496 [00:02<00:22, 20.02it/s] 11%|█▏        | 57/496 [00:02<00:24, 18.27it/s] 12%|█▏        | 60/496 [00:03<00:21, 20.09it/s] 13%|█▎        | 63/496 [00:03<00:22, 19.12it/s] 13%|█▎        | 66/496 [00:03<00:21, 20.00it/s] 14%|█▍        | 69/496 [00:03<00:22, 19.26it/s] 14%|█▍        | 71/496 [00:03<00:24, 17.57it/s] 15%|█▍        | 73/496 [00:03<00:25, 16.79it/s] 15%|█▌        | 75/496 [00:03<00:25, 16.71it/s] 16%|█▌        | 77/496 [00:04<00:24, 17.22it/s] 16%|█▌        | 79/496 [00:04<00:23, 17.90it/s] 16%|█▋        | 81/496 [00:04<00:24, 16.94it/s] 17%|█▋        | 83/496 [00:04<00:24, 16.59it/s] 17%|█▋        | 86/496 [00:04<00:21, 18.71it/s] 18%|█▊        | 89/496 [00:04<00:20, 19.76it/s] 18%|█▊        | 91/496 [00:04<00:21, 18.94it/s] 19%|█▉        | 93/496 [00:04<00:21, 18.85it/s] 19%|█▉        | 95/496 [00:05<00:22, 17.72it/s] 20%|█▉        | 98/496 [00:05<00:21, 18.75it/s] 20%|██        | 101/496 [00:05<00:19, 19.91it/s] 21%|██        | 103/496 [00:05<00:20, 18.80it/s] 21%|██        | 105/496 [00:05<00:20, 18.65it/s] 22%|██▏       | 108/496 [00:05<00:19, 19.98it/s] 22%|██▏       | 110/496 [00:05<00:20, 18.41it/s] 23%|██▎       | 113/496 [00:05<00:20, 18.81it/s] 23%|██▎       | 115/496 [00:06<00:20, 18.20it/s] 24%|██▎       | 117/496 [00:06<00:21, 17.57it/s] 24%|██▍       | 120/496 [00:06<00:19, 18.92it/s] 25%|██▍       | 122/496 [00:06<00:19, 18.88it/s] 25%|██▌       | 124/496 [00:06<00:19, 18.88it/s] 25%|██▌       | 126/496 [00:06<00:20, 18.10it/s] 26%|██▌       | 129/496 [00:06<00:19, 18.94it/s] 26%|██▋       | 131/496 [00:06<00:19, 19.16it/s] 27%|██▋       | 133/496 [00:07<00:18, 19.34it/s] 27%|██▋       | 135/496 [00:07<00:19, 18.81it/s] 28%|██▊       | 137/496 [00:07<00:20, 17.47it/s] 28%|██▊       | 139/496 [00:07<00:19, 17.98it/s] 29%|██▊       | 142/496 [00:07<00:18, 19.03it/s] 29%|██▉       | 144/496 [00:07<00:18, 19.26it/s] 29%|██▉       | 146/496 [00:07<00:18, 19.13it/s] 30%|██▉       | 148/496 [00:07<00:18, 18.66it/s] 30%|███       | 151/496 [00:08<00:17, 19.63it/s] 31%|███       | 153/496 [00:08<00:19, 17.45it/s] 31%|███▏      | 155/496 [00:08<00:20, 16.86it/s] 32%|███▏      | 157/496 [00:08<00:19, 16.97it/s] 32%|███▏      | 159/496 [00:08<00:19, 17.43it/s] 32%|███▏      | 161/496 [00:08<00:18, 17.68it/s] 33%|███▎      | 163/496 [00:08<00:18, 18.25it/s] 33%|███▎      | 166/496 [00:08<00:17, 19.15it/s] 34%|███▍      | 168/496 [00:08<00:17, 18.38it/s] 34%|███▍      | 170/496 [00:09<00:18, 17.67it/s] 35%|███▍      | 172/496 [00:09<00:18, 17.47it/s] 35%|███▌      | 174/496 [00:09<00:18, 17.82it/s] 35%|███▌      | 176/496 [00:09<00:17, 18.07it/s] 36%|███▌      | 178/496 [00:09<00:17, 18.26it/s] 36%|███▋      | 180/496 [00:09<00:17, 18.42it/s] 37%|███▋      | 182/496 [00:09<00:17, 17.88it/s] 37%|███▋      | 184/496 [00:09<00:17, 17.47it/s] 38%|███▊      | 186/496 [00:10<00:19, 15.81it/s] 38%|███▊      | 188/496 [00:10<00:19, 15.48it/s] 38%|███▊      | 190/496 [00:10<00:18, 16.43it/s] 39%|███▊      | 192/496 [00:10<00:18, 16.67it/s] 39%|███▉      | 194/496 [00:10<00:17, 17.35it/s] 40%|███▉      | 196/496 [00:10<00:16, 17.66it/s] 40%|████      | 199/496 [00:10<00:16, 18.09it/s] 41%|████      | 202/496 [00:10<00:15, 18.66it/s] 41%|████      | 204/496 [00:11<00:16, 17.87it/s] 42%|████▏     | 206/496 [00:11<00:16, 18.06it/s] 42%|████▏     | 209/496 [00:11<00:15, 18.42it/s] 43%|████▎     | 211/496 [00:11<00:15, 18.03it/s] 43%|████▎     | 214/496 [00:11<00:14, 19.23it/s] 44%|████▎     | 216/496 [00:11<00:14, 18.81it/s] 44%|████▍     | 218/496 [00:11<00:15, 18.35it/s] 44%|████▍     | 220/496 [00:11<00:15, 17.86it/s] 45%|████▍     | 222/496 [00:12<00:15, 17.80it/s] 45%|████▌     | 224/496 [00:12<00:15, 17.71it/s] 46%|████▌     | 226/496 [00:12<00:15, 17.89it/s] 46%|████▌     | 229/496 [00:12<00:14, 18.14it/s] 47%|████▋     | 231/496 [00:12<00:14, 17.82it/s] 47%|████▋     | 234/496 [00:12<00:14, 18.54it/s] 48%|████▊     | 236/496 [00:12<00:14, 18.07it/s] 48%|████▊     | 238/496 [00:12<00:14, 18.38it/s] 48%|████▊     | 240/496 [00:13<00:13, 18.39it/s] 49%|████▉     | 242/496 [00:13<00:14, 17.67it/s] 49%|████▉     | 244/496 [00:13<00:15, 16.78it/s] 50%|████▉     | 246/496 [00:13<00:14, 17.06it/s] 50%|█████     | 248/496 [00:13<00:13, 17.83it/s] 50%|█████     | 250/496 [00:13<00:14, 16.98it/s] 51%|█████     | 253/496 [00:13<00:13, 17.68it/s] 51%|█████▏    | 255/496 [00:13<00:13, 17.76it/s] 52%|█████▏    | 257/496 [00:14<00:13, 17.83it/s] 52%|█████▏    | 259/496 [00:14<00:13, 17.55it/s] 53%|█████▎    | 261/496 [00:14<00:14, 16.21it/s] 53%|█████▎    | 264/496 [00:14<00:12, 18.62it/s] 54%|█████▎    | 266/496 [00:14<00:12, 18.16it/s] 54%|█████▍    | 268/496 [00:14<00:12, 17.69it/s] 54%|█████▍    | 270/496 [00:14<00:12, 18.08it/s] 55%|█████▍    | 272/496 [00:14<00:13, 16.21it/s] 55%|█████▌    | 274/496 [00:15<00:13, 16.61it/s] 56%|█████▌    | 276/496 [00:15<00:15, 14.50it/s] 56%|█████▌    | 278/496 [00:15<00:15, 13.77it/s] 56%|█████▋    | 280/496 [00:15<00:15, 14.19it/s] 57%|█████▋    | 282/496 [00:15<00:14, 14.32it/s] 57%|█████▋    | 284/496 [00:15<00:13, 15.38it/s] 58%|█████▊    | 286/496 [00:15<00:13, 15.92it/s] 58%|█████▊    | 289/496 [00:15<00:11, 17.54it/s] 59%|█████▊    | 291/496 [00:16<00:12, 17.01it/s] 59%|█████▉    | 293/496 [00:16<00:11, 17.38it/s] 59%|█████▉    | 295/496 [00:16<00:11, 16.86it/s] 60%|█████▉    | 297/496 [00:16<00:13, 15.10it/s] 60%|██████    | 299/496 [00:16<00:13, 14.62it/s] 61%|██████    | 302/496 [00:16<00:11, 17.22it/s] 61%|██████▏   | 305/496 [00:16<00:10, 18.14it/s] 62%|██████▏   | 307/496 [00:17<00:10, 18.30it/s] 62%|██████▎   | 310/496 [00:17<00:09, 19.03it/s] 63%|██████▎   | 312/496 [00:17<00:09, 18.56it/s] 63%|██████▎   | 314/496 [00:17<00:09, 18.41it/s] 64%|██████▍   | 317/496 [00:17<00:09, 19.56it/s] 64%|██████▍   | 319/496 [00:17<00:09, 18.90it/s] 65%|██████▍   | 321/496 [00:17<00:09, 18.13it/s] 65%|██████▌   | 324/496 [00:17<00:09, 18.86it/s] 66%|██████▌   | 326/496 [00:18<00:10, 16.98it/s] 66%|██████▌   | 328/496 [00:18<00:09, 17.38it/s] 67%|██████▋   | 330/496 [00:18<00:09, 17.26it/s] 67%|██████▋   | 332/496 [00:18<00:10, 15.00it/s] 67%|██████▋   | 334/496 [00:18<00:10, 15.95it/s] 68%|██████▊   | 336/496 [00:18<00:10, 15.77it/s] 68%|██████▊   | 339/496 [00:18<00:08, 17.58it/s] 69%|██████▉   | 341/496 [00:18<00:08, 17.39it/s] 69%|██████▉   | 343/496 [00:19<00:08, 17.06it/s] 70%|██████▉   | 346/496 [00:19<00:08, 17.92it/s] 70%|███████   | 348/496 [00:19<00:08, 16.47it/s] 71%|███████   | 350/496 [00:19<00:08, 16.41it/s] 71%|███████   | 352/496 [00:19<00:08, 16.36it/s] 71%|███████▏  | 354/496 [00:19<00:08, 16.98it/s] 72%|███████▏  | 357/496 [00:19<00:07, 18.36it/s] 72%|███████▏  | 359/496 [00:20<00:07, 18.68it/s] 73%|███████▎  | 361/496 [00:20<00:07, 18.59it/s] 73%|███████▎  | 363/496 [00:20<00:07, 18.10it/s] 74%|███████▎  | 365/496 [00:20<00:07, 17.95it/s] 74%|███████▍  | 367/496 [00:20<00:07, 18.38it/s] 74%|███████▍  | 369/496 [00:20<00:07, 17.59it/s] 75%|███████▍  | 371/496 [00:20<00:07, 16.75it/s] 75%|███████▌  | 373/496 [00:20<00:07, 16.19it/s] 76%|███████▌  | 375/496 [00:20<00:07, 16.17it/s] 76%|███████▌  | 377/496 [00:21<00:06, 17.13it/s] 77%|███████▋  | 380/496 [00:21<00:06, 17.96it/s] 77%|███████▋  | 382/496 [00:21<00:06, 17.30it/s] 77%|███████▋  | 384/496 [00:21<00:06, 16.49it/s] 78%|███████▊  | 386/496 [00:21<00:06, 16.34it/s] 78%|███████▊  | 389/496 [00:21<00:05, 17.87it/s] 79%|███████▉  | 391/496 [00:21<00:06, 15.67it/s] 79%|███████▉  | 393/496 [00:22<00:06, 15.04it/s] 80%|███████▉  | 395/496 [00:22<00:06, 15.08it/s] 80%|████████  | 397/496 [00:22<00:06, 15.11it/s] 80%|████████  | 399/496 [00:22<00:06, 16.07it/s] 81%|████████  | 401/496 [00:22<00:05, 16.42it/s] 81%|████████▏ | 403/496 [00:22<00:05, 16.63it/s] 82%|████████▏ | 406/496 [00:22<00:05, 17.54it/s] 82%|████████▏ | 408/496 [00:22<00:04, 17.92it/s] 83%|████████▎ | 411/496 [00:23<00:04, 18.88it/s] 83%|████████▎ | 413/496 [00:23<00:04, 18.59it/s] 84%|████████▍ | 416/496 [00:23<00:04, 19.77it/s] 84%|████████▍ | 419/496 [00:23<00:03, 19.39it/s] 85%|████████▍ | 421/496 [00:23<00:03, 18.87it/s] 85%|████████▌ | 423/496 [00:23<00:04, 17.97it/s] 86%|████████▌ | 425/496 [00:23<00:04, 17.27it/s] 86%|████████▌ | 427/496 [00:23<00:03, 17.48it/s] 86%|████████▋ | 429/496 [00:24<00:03, 17.72it/s] 87%|████████▋ | 431/496 [00:24<00:03, 18.25it/s] 87%|████████▋ | 433/496 [00:24<00:03, 18.63it/s] 88%|████████▊ | 435/496 [00:24<00:03, 18.76it/s] 88%|████████▊ | 437/496 [00:24<00:03, 18.48it/s] 89%|████████▊ | 439/496 [00:24<00:03, 17.97it/s] 89%|████████▉ | 441/496 [00:24<00:03, 16.93it/s] 89%|████████▉ | 443/496 [00:24<00:03, 17.40it/s] 90%|████████▉ | 445/496 [00:24<00:03, 16.87it/s] 90%|█████████ | 448/496 [00:25<00:02, 18.22it/s] 91%|█████████ | 451/496 [00:25<00:02, 19.39it/s] 91%|█████████▏| 453/496 [00:25<00:02, 19.26it/s] 92%|█████████▏| 455/496 [00:25<00:02, 19.03it/s] 92%|█████████▏| 457/496 [00:25<00:02, 16.84it/s] 93%|█████████▎| 459/496 [00:25<00:02, 15.82it/s] 93%|█████████▎| 461/496 [00:25<00:02, 16.40it/s] 93%|█████████▎| 463/496 [00:26<00:02, 15.32it/s] 94%|█████████▍| 465/496 [00:26<00:01, 16.17it/s] 94%|█████████▍| 468/496 [00:26<00:01, 17.89it/s] 95%|█████████▍| 470/496 [00:26<00:01, 17.79it/s] 95%|█████████▌| 472/496 [00:26<00:01, 17.13it/s] 96%|█████████▌| 474/496 [00:26<00:01, 16.91it/s] 96%|█████████▌| 476/496 [00:26<00:01, 17.66it/s] 97%|█████████▋| 479/496 [00:26<00:00, 18.80it/s] 97%|█████████▋| 481/496 [00:27<00:00, 18.84it/s] 97%|█████████▋| 483/496 [00:27<00:00, 17.28it/s] 98%|█████████▊| 485/496 [00:27<00:00, 16.56it/s] 98%|█████████▊| 488/496 [00:27<00:00, 17.95it/s] 99%|█████████▉| 491/496 [00:27<00:00, 20.21it/s]100%|█████████▉| 494/496 [00:27<00:00, 18.67it/s][WARNING|training_args.py:959] 2021-12-17 21:23:15,516 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 496/496 [00:27<00:00, 17.84it/s]
